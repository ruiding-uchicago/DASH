{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aacfec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########import packages##########\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report\n",
    "from sklearn import ensemble\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import catboost\n",
    "import xgboost\n",
    "import lightgbm\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import pickle\n",
    "\n",
    "###########loading data##########\n",
    "loo = LeaveOneOut()\n",
    "seed=9239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28640cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########wrapping classification metrics for later calls##########\n",
    "def compute_classification_metrics(target, prediction):\n",
    "    accuracy = accuracy_score(target, prediction)\n",
    "    f1 = f1_score(target, prediction)\n",
    "    auc = roc_auc_score(target, prediction)\n",
    "    return accuracy, f1, auc\n",
    "\n",
    "def gridsearch(model, param, algorithm_name, X_train, y_train, X_test, y_test):\n",
    "    grid = GridSearchCV(model, param_grid=param, scoring='f1', cv=10, n_jobs=-1, verbose=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "    \n",
    "    prediction_train = best_model.predict(X_train)\n",
    "    prediction_test = best_model.predict(X_test)\n",
    "    \n",
    "    # Combine train and test predictions and true values\n",
    "    combined_predictions = np.concatenate([prediction_train, prediction_test])\n",
    "    combined_true_values = np.concatenate([y_train, y_test])\n",
    "    \n",
    "    accuracy_train, f1_train, auc_train = compute_classification_metrics(y_train, prediction_train)\n",
    "    accuracy_test, f1_test, auc_test = compute_classification_metrics(y_test, prediction_test)\n",
    "    accuracy_all, f1_all, auc_all = compute_classification_metrics(combined_true_values, combined_predictions)\n",
    "    \n",
    "    print(algorithm_name)\n",
    "    print('Best Classifier:', grid.best_params_)\n",
    "    print('--- Training Data ---')\n",
    "    print('Accuracy:', accuracy_train, 'F1:', f1_train, 'AUC:', auc_train)\n",
    "    print('--- Test Data ---')\n",
    "    print('Accuracy:', accuracy_test, 'F1:', f1_test, 'AUC:', auc_test)\n",
    "    print('--- All Data ---')\n",
    "    print('Accuracy:', accuracy_all, 'F1:', f1_all, 'AUC:', auc_all)\n",
    "    \n",
    "    return best_model, (accuracy_test+f1_test+auc_test)/3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b43574",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = open(r'./database_full_ac.pkl', 'rb')\n",
    "database_full = pickle.load(fl)\n",
    "##54 AC 55 ST\n",
    "data_input_full = database_full.iloc[:, 0:54]\n",
    "\n",
    "# Convert the target variable to binary based on the threshold\n",
    "threshold = 250\n",
    "data_output_full = (database_full.iloc[:, 54] < threshold).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_input_full, data_output_full, test_size=0.1, random_state=seed, stratify=data_output_full)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2956ffb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_RandomForestClassifier = RandomForestClassifier(random_state=1, verbose=0)\n",
    "###########defining the parameters dictionary##########\n",
    "param_RF= {\n",
    "'criterion': ['gini'], 'max_depth': [None], 'max_features': ['log2'], 'n_estimators': [200]}\n",
    "RF_full,RF_full_score=gridsearch(model_RandomForestClassifier, param_RF, 'Random Forest', X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad682e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LGBMClassifier=LGBMClassifier(random_state=1,verbose=0)\n",
    "param_lgbm = {\n",
    "'boosting_type': ['gbdt'], 'learning_rate': [0.18], 'max_depth': [11], 'n_estimators': [200], 'reg_alpha': [0.0001], 'reg_lambda': [0.001], 'subsample': [0.4]}\n",
    "LGBM_full,LGBM_full_score=gridsearch(model_LGBMClassifier,param_lgbm,'LightGBM',X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7614be0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###########RandomForest gridsearch CV for best hyperparameter##########\n",
    "model_XGClassifier = XGBClassifier(random_state=1)\n",
    "###########defining the parameters dictionary##########\n",
    "param_XG = {\n",
    "'booster': ['gbtree'], 'learning_rate': [0.08], 'max_depth': [11], 'n_estimators': [200], 'reg_alpha': [1e-05], 'reg_lambda': [0], 'subsample': [0.7]}\n",
    "XG_full,XG_full_score=gridsearch(model_XGClassifier,param_XG,'XGBoost',X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053b1121",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb8e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pdpbox.pdp_calc_utils import _calc_ice_lines_inter\n",
    "from pdpbox.pdp import pdp_isolate, PDPInteract\n",
    "from pdpbox.utils import (_check_model, _check_dataset, _check_percentile_range, _check_feature,\n",
    "                    _check_grid_type, _check_memory_limit, _make_list,\n",
    "                    _calc_memory_usage, _get_grids, _get_grid_combos, _check_classes)\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def pdp_multi_interact(model, dataset, model_features, features, \n",
    "                    num_grid_points=None, grid_types=None, percentile_ranges=None, grid_ranges=None, cust_grid_points=None, \n",
    "                    cust_grid_combos=None, use_custom_grid_combos=False,\n",
    "                    memory_limit=0.9, n_jobs=8, predict_kwds=None, data_transformer=None):\n",
    "\n",
    "    def _expand_default(x, default, length):\n",
    "        if x is None:\n",
    "            return [default] * length\n",
    "        return x\n",
    "\n",
    "    def _get_grid_combos(feature_grids, feature_types):\n",
    "        grids = [np.array(list(feature_grid),dtype=np.float16) for feature_grid in feature_grids]\n",
    "        for i in range(len(feature_types)):\n",
    "            if feature_types[i] == 'onehot':\n",
    "                grids[i] = np.eye(len(grids[i])).astype(int).tolist()\n",
    "        return np.stack(np.meshgrid(*grids,copy=bool), -1).reshape(-1, len(grids))\n",
    "\n",
    "    if predict_kwds is None:\n",
    "        predict_kwds = dict()\n",
    "\n",
    "    nr_feats = len(features)\n",
    "\n",
    "    # check function inputs\n",
    "    n_classes, predict = _check_model(model=model)\n",
    "    _check_dataset(df=dataset)\n",
    "    _dataset = dataset.copy()\n",
    "\n",
    "    # prepare the grid\n",
    "    pdp_isolate_outs = []\n",
    "    if use_custom_grid_combos:\n",
    "        grid_combos = cust_grid_combos\n",
    "        feature_grids = []\n",
    "        feature_types = []\n",
    "    else:\n",
    "        num_grid_points = _expand_default(x=num_grid_points, default=10, length=nr_feats)\n",
    "        grid_types = _expand_default(x=grid_types, default='percentile', length=nr_feats)\n",
    "        for i in range(nr_feats):\n",
    "            _check_grid_type(grid_type=grid_types[i])\n",
    "\n",
    "        percentile_ranges = _expand_default(x=percentile_ranges, default=None, length=nr_feats)\n",
    "        for i in range(nr_feats):\n",
    "            _check_percentile_range(percentile_range=percentile_ranges[i])\n",
    "\n",
    "        grid_ranges = _expand_default(x=grid_ranges, default=None, length=nr_feats)\n",
    "        cust_grid_points = _expand_default(x=cust_grid_points, default=None, length=nr_feats)\n",
    "\n",
    "        _check_memory_limit(memory_limit=memory_limit)\n",
    "\n",
    "        pdp_isolate_outs = []\n",
    "        for idx in range(nr_feats):\n",
    "            pdp_isolate_out = pdp_isolate(\n",
    "                model=model, dataset=_dataset, model_features=model_features, feature=features[idx],\n",
    "                num_grid_points=num_grid_points[idx], grid_type=grid_types[idx], percentile_range=percentile_ranges[idx],\n",
    "                grid_range=grid_ranges[idx], cust_grid_points=cust_grid_points[idx], memory_limit=memory_limit,\n",
    "                n_jobs=n_jobs, predict_kwds=predict_kwds, data_transformer=data_transformer)\n",
    "            pdp_isolate_outs.append(pdp_isolate_out)\n",
    "\n",
    "        if n_classes > 2:\n",
    "            feature_grids = [pdp_isolate_outs[i][0].feature_grids for i in range(nr_feats)]\n",
    "            feature_types = [pdp_isolate_outs[i][0].feature_type  for i in range(nr_feats)]\n",
    "        else:\n",
    "            feature_grids = [pdp_isolate_outs[i].feature_grids for i in range(nr_feats)]\n",
    "            feature_types = [pdp_isolate_outs[i].feature_type  for i in range(nr_feats)]\n",
    "\n",
    "        grid_combos = _get_grid_combos(feature_grids, feature_types)\n",
    "\n",
    "    feature_list = []\n",
    "    for i in range(nr_feats):\n",
    "        feature_list.extend(_make_list(features[i]))\n",
    "\n",
    "    # Parallel calculate ICE lines\n",
    "    true_n_jobs = _calc_memory_usage(\n",
    "        df=_dataset, total_units=len(grid_combos), n_jobs=n_jobs, memory_limit=memory_limit)\n",
    "\n",
    "    grid_results = Parallel(n_jobs=true_n_jobs)(delayed(_calc_ice_lines_inter)(\n",
    "        grid_combo, data=_dataset, model=model, model_features=model_features, n_classes=n_classes,\n",
    "        feature_list=feature_list, predict_kwds=predict_kwds, data_transformer=data_transformer)\n",
    "                                                for grid_combo in grid_combos)\n",
    "\n",
    "    ice_lines = pd.concat(grid_results, axis=0).reset_index(drop=True)\n",
    "    pdp = ice_lines.groupby(feature_list, as_index=False).mean()\n",
    "\n",
    "    # combine the final results\n",
    "    pdp_interact_params = {'n_classes': n_classes, \n",
    "                        'features': features, \n",
    "                        'feature_types': feature_types,\n",
    "                        'feature_grids': feature_grids}\n",
    "    if n_classes > 2:\n",
    "        pdp_interact_out = []\n",
    "        for n_class in range(n_classes):\n",
    "            _pdp = pdp[feature_list + ['class_%d_preds' % n_class]].rename(\n",
    "                columns={'class_%d_preds' % n_class: 'preds'})\n",
    "            pdp_interact_out.append(\n",
    "                PDPInteract(which_class=n_class,\n",
    "                            pdp_isolate_outs=[pdp_isolate_outs[i][n_class] for i in range(nr_feats)],\n",
    "                            pdp=_pdp, **pdp_interact_params))\n",
    "    else:\n",
    "        pdp_interact_out = PDPInteract(\n",
    "            which_class=None, pdp_isolate_outs=pdp_isolate_outs, pdp=pdp, **pdp_interact_params)\n",
    "\n",
    "    return pdp_interact_out\n",
    "print('ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a368ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center(arr): return arr - np.mean(arr)\n",
    "import itertools\n",
    "def compute_f_vals(mdl, X, features, selectedfeatures, num_grid_points=10, use_data_grid=False):\n",
    "    f_vals = {}\n",
    "    data_grid = None\n",
    "    if use_data_grid:\n",
    "        data_grid = X[selectedfeatures].values\n",
    "    # Calculate partial dependencies for full feature set\n",
    "    p_full = pdp_multi_interact(mdl, X, features, selectedfeatures, \n",
    "                                num_grid_points=[num_grid_points] * len(selectedfeatures),\n",
    "                                cust_grid_combos=data_grid,\n",
    "                                use_custom_grid_combos=use_data_grid)\n",
    "    f_vals[tuple(selectedfeatures)] = center(p_full.pdp.preds.values)\n",
    "    grid = p_full.pdp.drop('preds', axis=1)\n",
    "    # Calculate partial dependencies for [1..SFL-1]\n",
    "    for n in range(1, len(selectedfeatures)):\n",
    "        for subsetfeatures in itertools.combinations(selectedfeatures, n):\n",
    "            if use_data_grid:\n",
    "                data_grid = X[list(subsetfeatures)].values\n",
    "            p_partial = pdp_multi_interact(mdl, X, features, subsetfeatures, \n",
    "                                        num_grid_points=[num_grid_points] * len(selectedfeatures),\n",
    "                                        cust_grid_combos=data_grid,\n",
    "                                        use_custom_grid_combos=use_data_grid)\n",
    "            p_joined = pd.merge(grid, p_partial.pdp, how='left')\n",
    "            f_vals[tuple(subsetfeatures)] = center(p_joined.preds.values)\n",
    "    return f_vals\n",
    "def compute_h_val(f_vals, selectedfeatures):\n",
    "    denom_els = f_vals[tuple(selectedfeatures)].copy()\n",
    "    numer_els = f_vals[tuple(selectedfeatures)].copy()\n",
    "    sign = -1.0\n",
    "    for n in range(len(selectedfeatures)-1, 0, -1):\n",
    "        for subfeatures in itertools.combinations(selectedfeatures, n):\n",
    "            print(tuple(subfeatures))\n",
    "            numer_els += sign * f_vals[tuple(subfeatures)]\n",
    "        sign *= -1.0\n",
    "    numer = np.sum(numer_els**2)\n",
    "    denom = np.sum(denom_els**2)\n",
    "    return math.sqrt(numer/denom) if numer < denom else np.nan\n",
    "def compute_h_val_any(f_vals, allfeatures, selectedfeature):\n",
    "    otherfeatures = list(allfeatures)\n",
    "    otherfeatures.remove(selectedfeature)\n",
    "    denom_els = f_vals[tuple(allfeatures)].copy()\n",
    "    numer_els = denom_els.copy()\n",
    "    numer_els -= f_vals[(selectedfeature,)]\n",
    "    numer_els -= f_vals[tuple(otherfeatures)]\n",
    "    numer = np.sum(numer_els**2)\n",
    "    denom = np.sum(denom_els**2)\n",
    "    return math.sqrt(numer/denom) if numer < denom else np.nan\n",
    "def compute_interactions(model,X_train,feature_all,feature_select_list):  \n",
    "    result_dict={}\n",
    "    for i in range(len(feature_select_list)):\n",
    "        for j in range(len(feature_select_list)):\n",
    "            if i<j :\n",
    "                print(i,j)\n",
    "                try:\n",
    "                    current_features=[feature_select_list[i],feature_select_list[j]]\n",
    "                    f_vals=compute_f_vals(model, X_train, feature_all,current_features) \n",
    "                    result_dict[tuple(current_features)]=compute_h_val(f_vals,current_features)\n",
    "                except:\n",
    "                    result_dict[tuple(current_features)]=0\n",
    "                print(result_dict[tuple(current_features)])\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df5e989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RF_DICT=compute_interactions(RF_full,data_input_full,data_input_full.columns,list(data_input_full.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61434325",
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBM_DICT=compute_interactions(LGBM_full,data_input_full,data_input_full.columns,list(data_input_full.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cbbcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "XG_DICT=compute_interactions(XG_full,data_input_full,data_input_full.columns,list(data_input_full.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4613ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_matrix_weighted(target_dict,target_score):\n",
    "    df=pd.DataFrame(columns=data_input_full.columns,index=data_input_full.columns)\n",
    "    for each in target_dict:\n",
    "        df.loc[each[0],each[1]]=target_dict[each]*target_score\n",
    "        df.loc[each[1],each[0]]=target_dict[each]*target_score\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7465a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_DF=construct_matrix_weighted(RF_DICT,RF_full_score)\n",
    "XG_DF=construct_matrix_weighted(XG_DICT,XG_full_score)\n",
    "LGBM_DF=construct_matrix_weighted(LGBM_DICT,LGBM_full_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70403616",
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBM_DF.to_csv('INTERACTION_LGBM_FULL.csv')\n",
    "RF_DF.to_csv('INTERACTION_RF_FULL.csv')\n",
    "XG_DF.to_csv('INTERACTION_XG_FULL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ff7b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBM_DF=pd.read_csv('INTERACTION_LGBM_FULL.csv',index_col=0)\n",
    "RF_DF=pd.read_csv('INTERACTION_RF_FULL.csv',index_col=0)\n",
    "XG_DF=pd.read_csv('INTERACTION_XG_FULL.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b814b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Weighted_Matrix=(XG_DF+RF_DF+LGBM_DF)/(XG_full_score+RF_full_score+LGBM_full_score)\n",
    "Weighted_Matrix=Weighted_Matrix.fillna(0)\n",
    "Weighted_Matrix=Weighted_Matrix/Weighted_Matrix.max().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43fcc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "Weighted_Matrix.to_csv('INTERACTION_FULL_WEIGHTED.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0cbd74",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "f, ax= plt.subplots(figsize = (16, 16))\n",
    "sns.set(font_scale=1)\n",
    "ax=sns.heatmap(Weighted_Matrix,annot=False, vmax=1,vmin = 0, xticklabels= True, yticklabels= True, square=True, cmap=\"gist_heat_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6209a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Element_M=Weighted_Matrix.iloc[0:36,0:36]\n",
    "Synthesis_M=Weighted_Matrix.iloc[36:55,36:55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96225710",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1, ax1= plt.subplots(figsize = (16, 16))\n",
    "# sns.set(font_scale=2)\n",
    "ax1=sns.heatmap(Element_M,annot=False, vmax=1,vmin = 0, xticklabels= True, yticklabels= True, square=True, cmap=\"gist_heat_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfc9c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2, ax2= plt.subplots(figsize = (16, 16))\n",
    "# sns.set(font_scale=2)\n",
    "ax2=sns.heatmap(Synthesis_M,annot=False, vmax=1,vmin = 0, xticklabels= True, yticklabels= True, square=True, cmap=\"gist_heat_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0be41df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e19e033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
