{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f861c802",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########import packages##########\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import ensemble\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import numpy as np\n",
    "from sko.GA import GA\n",
    "import time\n",
    "import datetime\n",
    "from sko.tools import set_run_mode\n",
    "from numpy import *\n",
    "seed=77\n",
    "###########import packages##########\n",
    "import catboost\n",
    "import xgboost\n",
    "import lightgbm\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import *\n",
    "import pickle\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import  *\n",
    "###########import packages##########\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.constraints import max_norm\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense \n",
    "from keras.layers import Dropout \n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasClassifier \n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.constraints import maxnorm \n",
    "# from keras.wrappers import scikit_learn\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "###########loading data##########\n",
    "loo = LeaveOneOut()\n",
    "# %matplotlib\n",
    "###########wrapping root mean square error for later calls##########\n",
    "def compute_mae_mse_rmse(target,prediction):\n",
    "    error = []\n",
    "    for i in range(len(target)):\n",
    "        error.append(target[i] - prediction[i])\n",
    "    squaredError = []\n",
    "    absError = []\n",
    "    for val in error:\n",
    "        squaredError.append(val * val)  # target-prediction之差平方\n",
    "        absError.append(abs(val))  # 误差绝对值\n",
    "    mae=sum(absError)/len(absError)  # 平均绝对误差MAE\n",
    "    mse=sum(squaredError)/len(squaredError)  # 均方误差MSE\n",
    "    RMSE=np.sqrt(sum(squaredError)/len(squaredError))\n",
    "    R2=r2_score(target,prediction)\n",
    "    return mae,mse,RMSE,R2\n",
    "def gridsearch(model,param,algorithm_name,X,y):\n",
    "#     grid = GridSearchCV(model,param_grid=param,scoring='neg_mean_absolute_error',cv=10,n_jobs=-1,verbose=2)\n",
    "    grid = GridSearchCV(model,param_grid=param,scoring='neg_mean_absolute_error',cv=10,n_jobs=-1,verbose=0)\n",
    "    grid.fit(X,y)\n",
    "    best_model=grid.best_estimator_\n",
    "    \n",
    "    prediction_all = best_model.predict(X)\n",
    "    real_all=y.values\n",
    "    prediction_all_series=pd.Series(prediction_all)\n",
    "    real_all_series=pd.Series(real_all)\n",
    "    \n",
    "    ###########evaluating the regression quality##########\n",
    "    corr_ann = round(prediction_all_series.corr(real_all_series), 5)\n",
    "    error_val= compute_mae_mse_rmse(prediction_all,real_all)\n",
    "    \n",
    "    print(algorithm_name)\n",
    "    best_score=grid.best_score_\n",
    "    print('Best Regressor:',grid.best_params_,'Best Score:', best_score)\n",
    "\n",
    "    return best_model,best_score\n",
    "\n",
    "fl = open(r'./database_full_ac.pkl','rb')\n",
    "database_full=pickle.load(fl)\n",
    "data_input_full=database_full.iloc[:,0:54]\n",
    "data_output_full=database_full.iloc[:,54]\n",
    "X_train,X_test,y_train,y_test=train_test_split(data_input_full,data_output_full,test_size=0.1,random_state=seed)\n",
    "\n",
    "model_SVR = svm.SVR()\n",
    "param_svr = {\n",
    "'kernel':['linear', 'poly', 'rbf'],\n",
    "'max_iter':[100,200,300,400,500,600,700,800,1000,1100,1200,1300,1400,1500],\n",
    "'degree':[2,3,4],\n",
    "'gamma':['scale','auto'],\n",
    "'epsilon':[0.001,0.01,0.1,0.3,0.5,0.7,1],\n",
    "'coef0':[100,200,300,400,500,600,700,800,1000,1100,1200,1300,1400,1500]\n",
    "       }\n",
    "SVR_full,SVR_full_score=gridsearch(model_SVR,param_svr,'Support Vector Regressor',data_input_full,data_output_full)\n",
    "\n",
    "model_KNeighborsRegressor = neighbors.KNeighborsRegressor()\n",
    "param_knr = {\n",
    "'n_neighbors':range(1,10),'weights':['uniform','distance'],\n",
    " 'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "'leaf_size':[2,10,20,30,40,50,100],\n",
    "'p':range(1,10)\n",
    "       }\n",
    "KNR_full,KNR_full_score=gridsearch(model_KNeighborsRegressor,param_knr,'K Nearest Neighbor Regressor',data_input_full,data_output_full)\n",
    "\n",
    "model_LGBMRegressor=LGBMRegressor(random_state=1,verbose=0)\n",
    "param_lgbm = {\n",
    "'boosting_type':['gbdt','rf'],\n",
    "'learning_rate':[0.001,0.002,0.004,0.005,0.006,0.008,0.01,0.02,0.04,0.06,0.05,0.06,0.08,0.1,0.12,0.14,0.15,0.16,0.18,0.2,0.4,0.5,0.6,0.8,1],\n",
    "'subsample':[0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1],\n",
    "'n_estimators':[50,100,200,400],\n",
    "'max_depth':[5,7,9,11,13,-1],\n",
    "'reg_alpha':[0,0.001,0.01,0.0001,0.00001],\n",
    "'reg_lambda':[0,0.001,0.01,0.0001,0.00001]\n",
    "}\n",
    "LGBM_full,LGBM_full_score=gridsearch(model_LGBMRegressor,param_lgbm,'LightGBM',data_input_full,data_output_full)\n",
    "\n",
    "model_XGRegressor=XGBRegressor(random_state=1)\n",
    "param_xg={\n",
    "'booster':['gbtree'],\n",
    "'learning_rate':[0.001,0.002,0.004,0.005,0.006,0.008,0.01,0.02,0.04,0.06,0.05,0.06,0.08,0.1,0.12,0.14,0.15,0.16,0.18,0.2,0.4,0.5,0.6,0.8,1],\n",
    "'n_estimators':[100,200,400],\n",
    "'max_depth':[3,7,9,11,13,-1],\n",
    "'subsample':[0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1],\n",
    "'reg_alpha':[0,0.001,0.01,0.0001,0.00001],\n",
    "'reg_lambda':[0,0.001,0.01,0.0001,0.00001]\n",
    "}\n",
    "XG_full,XG_full_score=gridsearch(model_XGRegressor,param_xg,'XGBoost',data_input_full,data_output_full)\n",
    "\n",
    "model_CatRegressor=catboost.CatBoostRegressor(random_state=1,verbose=0)\n",
    "param_cat = {\n",
    "'learning_rate':[0.001,0.002,0.004,0.005,0.006,0.008,0.01,0.02,0.04,0.06,0.05,0.06,0.08,0.1,0.12,0.14,0.15,0.16,0.18,0.2],\n",
    "'n_estimators':[100,200,400],\n",
    "\"boosting_type\":[\"Plain\"],\n",
    "'max_depth':[5,7,9,11],\n",
    "'subsample':[0.4,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1],\n",
    "'reg_lambda':[0,0.001,0.01,0.0001,0.00001]\n",
    "}\n",
    "CAT_full,CAT_full_score=gridsearch(model_CatRegressor,param_cat,'CatBoost',data_input_full,data_output_full)\n",
    "\n",
    "model_GradientBoostingRegressor = ensemble.GradientBoostingRegressor(random_state=1)\n",
    "param_GB = {\n",
    "'learning_rate':[0.001,0.002,0.004,0.005,0.006,0.008,0.01,0.02,0.04,0.06,0.05,0.06,0.08,0.1,0.12,0.14,0.15,0.16,0.18,0.2,0.4,0.5,0.6,0.8,1],\n",
    "'n_estimators':[50,100,200,400],\n",
    "'max_depth':[3,5,7,9,11,13,16],\n",
    "'criterion':['friedman_mse','mae','mse'],\n",
    "'max_features':['auto','sqrt','log2'],\n",
    "'loss':['ls', 'lad', 'huber', 'quantile']\n",
    "}\n",
    "GB_full,GB_full_score=gridsearch(model_GradientBoostingRegressor,param_GB,'GradientBoost',data_input_full,data_output_full)\n",
    "\n",
    "model_RandomForestRegressor = ensemble.RandomForestRegressor(random_state=1)\n",
    "param_RF = {\n",
    "'n_estimators':[50,100,200,400,None],\n",
    "'max_depth':[3,5,7,9,11,None],\n",
    "'criterion':['mse','mae'],\n",
    "'max_features':['auto','sqrt','log2']\n",
    "}\n",
    "RF_full,RF_full_score=gridsearch(model_RandomForestRegressor,param_RF,'Random Forest',data_input_full,data_output_full)\n",
    "\n",
    "\n",
    "model_DecisionTreeRegressor = tree.DecisionTreeRegressor(random_state=1)\n",
    "param_dt={\n",
    "'max_depth':[5,6,7,8,9,10,11,None],\n",
    "'max_features':['auto','sqrt','log2'],\n",
    "'criterion' : [\"mse\", \"friedman_mse\", \"mae\"],\n",
    "'splitter' : [ \"best\",'random']\n",
    "}\n",
    "DT_full,DT_full_score=gridsearch(model_DecisionTreeRegressor,param_dt,'Decision Tree',data_input_full,data_output_full)\n",
    "\n",
    "model_AdaBoostRegressor = ensemble.AdaBoostRegressor(random_state=1)\n",
    "param_ada={\n",
    "'n_estimators':[50,100,200,400,800],\n",
    "'learning_rate':[0.001,0.002,0.004,0.005,0.006,0.008,0.01,0.02,0.04,0.06,0.05,0.06,0.08,0.1,0.12,0.14,0.15,0.16,0.18,0.2,0.4,0.5,0.6,0.8,1],\n",
    "'loss':['linear', 'square', 'exponential']  \n",
    "}\n",
    "ADA_full,ADA_full_score=gridsearch(model_AdaBoostRegressor,param_ada,'AdaBoost',data_input_full,data_output_full)\n",
    "\n",
    "\n",
    "def create_ANN_model_1layer(X,learning_rate,regular_term=0.001,neuron_number=50,drop_out_rate=0):\n",
    "    regularizer=keras.regularizers.l2(regular_term)\n",
    "    model = Sequential() \n",
    "    model.add(Dense(neuron_number, input_dim=X.shape[1], kernel_initializer='random_normal',\n",
    "                    bias_initializer='random_normal',activation='relu',kernel_regularizer=regularizer)) \n",
    "    model.add(Dropout(drop_out_rate))\n",
    "    model.add(Dense(neuron_number, input_dim=neuron_number, kernel_initializer='random_normal',\n",
    "                    bias_initializer='random_normal',activation='relu',kernel_regularizer=regularizer)) \n",
    "    model.add(Dropout(drop_out_rate))\n",
    "    model.add(Dense(1, input_dim=neuron_number, activation='linear'))\n",
    "    adam=optimizers.Adam(learning_rate)\n",
    "    model.compile(loss='mae')\n",
    "    return model\n",
    "def create_ANN_model_2layer(X,learning_rate,regular_term=0.001,neuron_number=50,drop_out_rate=0):\n",
    "    regularizer=keras.regularizers.l2(regular_term)\n",
    "    model = Sequential() \n",
    "    model.add(Dense(neuron_number, input_dim=X.shape[1], kernel_initializer='random_normal',\n",
    "                    bias_initializer='random_normal',activation='relu',kernel_regularizer=regularizer)) \n",
    "    model.add(Dropout(drop_out_rate))\n",
    "    model.add(Dense(neuron_number, input_dim=neuron_number, kernel_initializer='random_normal',\n",
    "                    bias_initializer='random_normal',activation='relu',kernel_regularizer=regularizer)) \n",
    "    model.add(Dropout(drop_out_rate))\n",
    "    model.add(Dense(neuron_number, input_dim=neuron_number, kernel_initializer='random_normal',\n",
    "                    bias_initializer='random_normal',activation='relu',kernel_regularizer=regularizer)) \n",
    "    model.add(Dropout(drop_out_rate))\n",
    "    model.add(Dense(1, input_dim=neuron_number, activation='linear'))\n",
    "    adam=optimizers.Adam(learning_rate)\n",
    "    model.compile(loss='mae')\n",
    "    return model\n",
    "\n",
    "model_ANNRegressor1= KerasRegressor(build_fn=create_ANN_model_1layer(X=data_input_full,learning_rate=0.01), verbose=0)\n",
    "model_ANNRegressor2= KerasRegressor(build_fn=create_ANN_model_2layer(X=data_input_full,learning_rate=0.01), verbose=0)\n",
    "\n",
    "\n",
    "epochs_list=[]\n",
    "for i in range(10,210,10):\n",
    "    epochs_list.append(i)\n",
    "# 设置参数候选值\n",
    "batch_size_list = [8,16,32]\n",
    "optimizers_list=['sgd', 'rmsprop', 'adam', 'adagrad']\n",
    "param_ann = dict(batch_size=batch_size_list, \n",
    "                 epochs=epochs_list,\n",
    "                optimizer=optimizers_list\n",
    "                )\n",
    "ANN_1layer,ANN_1layer_score=gridsearch(model_ANNRegressor1,param_ann,'Artificial Neural Network',data_input_full,data_output_full)\n",
    "ANN_2layer,ANN_2layer_score=gridsearch(model_ANNRegressor2,param_ann,'Artificial Neural Network',data_input_full,data_output_full)\n",
    "\n",
    "###FIRST PRINCIPAL ELEMENTAL PROPERTIES AND DESCRIPTORS\n",
    "element_information={}\n",
    "element_information['None']=[0,0,0,0,0,0,0,0]\n",
    "element_information['Ru']=[101.07, 44, 5, 8, 7.5, 2.2, 7, 134]\n",
    "element_information['Ir']=[192.217, 77, 6, 9, 9.1, 2.2, 7, 136]\n",
    "element_information['Mn']=[54.938, 25, 4, 7, 7.4, 1.56, 5, 127]\n",
    "element_information['Ba']=[137.327, 56, 6, 2, 5.19, 0.89, 10, 222]\n",
    "element_information['Sr']=[87.62, 38, 5, 2, 5.67, 0.95, 10, 215]\n",
    "element_information['Na']=[22.9897, 11, 3, 1, 5.12, 0.93, 0, 190]\n",
    "element_information['Ag']=[107.868, 47, 5, 11, 7.54, 1.93, 10, 144]\n",
    "element_information['La']=[138.905, 57, 6, 3, 5.5, 1.1, 1, 187]\n",
    "element_information['Zn']=[65.38, 30, 4, 12, 9.35, 1.65, 10, 138]\n",
    "element_information['K']=[39.0983, 19, 4, 1, 4.32, 0.82, 0, 235] \n",
    "element_information['Al']=[26.9815, 13, 3, 13, 5.95, 1.61, 0,143]\n",
    "element_information['Au']=[196.966, 79, 6, 11, 9.19, 2.54, 10, 144]\n",
    "element_information['Pr']=[140.904, 59, 6, 3, 5.76, 1.13, 10, 182]\n",
    "element_information['Nb']=[92.906, 41, 5, 5, 6.76, 1.6, 4, 146]\n",
    "element_information['Li']=[6.941, 3, 2, 1, 5.37, 0.98, 0, 145]\n",
    "element_information['Ca']=[40.078, 20, 4, 2, 6.09, 1, 0, 197]\n",
    "element_information['Cr']=[51.996, 24, 4, 6, 6.74, 1.66, 4, 130]\n",
    "element_information['In']=[114.818, 49, 5, 13, 8.95, 1.78, 10, 166]\n",
    "element_information['Nd']=[144.242, 60, 6, 3, 6.31, 1.14, 10, 182]\n",
    "element_information['Mo']=[95.94, 42, 5, 6, 7.35, 2.16, 5, 139]\n",
    "element_information['Ti']=[47.867, 22, 4, 4, 6.81, 1.54, 2, 147]\n",
    "element_information['W']=[183.84, 74, 6, 6, 7.98, 2.36, 4, 141]\n",
    "element_information['Zr']=[91.224, 40, 5, 4, 6, 1.33, 2, 160]\n",
    "element_information['Ce']=[140.116, 58, 6, 3, 6.91, 1.12, 1, 181]\n",
    "element_information['Re']=[186.207, 75, 6, 7, 7.88, 1.9, 5, 137]\n",
    "element_information['Ta']=[180.947,73, 6, 5, 7.89, 1.5, 3, 149]\n",
    "element_information['Gd']=[157.25, 64, 6, 3, 6.65, 1.2, 1, 179]\n",
    "element_information['F']=[18.9984, 9, 2, 17, 18.6, 3.98, 0, 73]\n",
    "element_information['Sm']=[150.36, 62, 6, 3, 6.55, 1.1, 10, 181]\n",
    "element_information['N']=[14.0067, 7, 2, 15, 14.48, 3.04, 0, 92]\n",
    "element_information['Er']=[167.529, 68, 6, 3, 6.108, 1.23, 10, 178]\n",
    "element_information['Sn']=[118.71, 50, 5, 14, 7.37, 1.96, 10, 162]\n",
    "element_information['Pd']=[106.42, 46, 5, 10, 8.3, 2.2, 10, 137]\n",
    "element_information['Ni']=[58.6934, 28, 4, 10, 7.61, 1.91, 8, 124]\n",
    "#################################################################\n",
    "element_information['Sc']=[44.956, 21, 4, 3, 6.57, 1.36, 1, 162]\n",
    "element_information['V']=[50.942, 23, 4, 5, 6.76, 1.63, 3, 134]\n",
    "element_information['Fe']=[55.845, 26, 4, 8, 7.83, 1.83, 6, 126]\n",
    "element_information['Co']=[58.933, 27, 4, 9, 7.81, 1.88, 7, 125]\n",
    "element_information['Cu']=[63.546, 29, 4, 11, 7.69, 1.9, 10, 128]\n",
    "element_information['Ga']=[69.723, 31, 4, 13, 5.97, 1.81, 10, 141]\n",
    "element_information['Y']=[88.905, 39, 5, 3, 6.5, 1.22, 1, 178]\n",
    "element_information['Mo']=[95.94, 42, 5, 6, 7.35, 2.16, 5, 139]\n",
    "element_information['Tc']=[98.906, 43, 5, 7, 7.28, 1.9, 5, 136]\n",
    "element_information['Rh']=[102.905, 45, 5, 9, 7.7, 2.28, 8, 134]\n",
    "element_information['Cd']=[112.411, 48, 5, 12, 8.95, 1.69, 10, 154]\n",
    "element_information['Pm']=[144.912, 61, 6, 3 ,5.55, 1.13, 10, 183]\n",
    "element_information['Eu']=[151.964, 63, 6, 3, 5.67, 1.2, 10, 199]\n",
    "element_information['Tb']=[158.925, 65, 6, 3, 6.74, 1.2, 10, 180]\n",
    "element_information['Dy']=[162.5, 66, 6, 3, 6.82, 1.22, 10, 180]\n",
    "element_information['Ho']=[164.93, 67, 6, 3, 6.022, 1.23, 10, 179]\n",
    "element_information['Tm']=[168.934, 69, 6, 3, 6.184, 1.25, 10, 177]\n",
    "element_information['Yb']=[173.04, 70, 6, 3, 7.06, 1.1, 10, 176]\n",
    "element_information['Lu']=[174.967, 71, 6, 3, 5.4259, 1.27, 1, 175]\n",
    "element_information['Hf']=[178.49, 72, 6, 4, 6.8251, 1.3, 2, 167]\n",
    "element_information['Os']=[190.23, 76, 6, 8, 8.7, 2.2, 6, 135]\n",
    "element_information['Pt']=[195.084, 78, 6, 10, 8.9, 2.28, 9, 139]\n",
    "element_information['Hg']=[200.59, 80, 6, 12, 10.39, 2, 10, 157]\n",
    "element_information['Tl']=[204.383, 81, 6, 13, 6.08, 1.62, 10, 171]\n",
    "element_information['Pb']=[207.2, 82, 6, 14, 7.38, 2.33, 10, 175]\n",
    "element_information['Bi']=[208.98, 83, 6, 15, 7.25, 2.02, 10, 170]\n",
    "element_information['Mg']=[24.3050, 12, 3, 2, 7.61, 1.31, 0, 160]\n",
    "element_information['C']=[12.0107, 6, 2, 14, 11.22, 2.56, 0, 77]\n",
    "element_information['B']=[10.811, 5, 2, 13, 8.33, 2.04, 0, 98]\n",
    "element_information['P']=[30.9737, 15, 3, 15, 10.3, 2.19, 0, 128]\n",
    "element_information['S']=[32.065, 16, 3, 16, 10.31, 2.58, 0, 127]\n",
    "element_information['Sb']=[121.760, 51, 5, 15, 8.35, 2.05, 10, 159]\n",
    "element_information['Te']=[127.6, 52, 5, 16, 9.0096, 2.1, 10, 160]\n",
    "element_information['Br']=[79.904, 35, 4, 17, 11.8, 2.96, 10, 115]\n",
    "element_information['Cl']=[35.453, 17, 3, 17, 12.96, 3.16, 0, 99]\n",
    "element_information['Si']=[28.0855, 14, 3, 14, 8.12, 1.9, 0, 132]\n",
    "element_information['Se']=[78.96, 34, 4, 16, 9.5, 2.55, 10, 140]\n",
    "element_list=list(element_information.keys())\n",
    "\n",
    "########HERE INTRODUCE THE LIMITATION FOR SOME CONDITIONS####\n",
    "def equa_element_2(x):\n",
    "    return ((x[1]==0 and x[5]==0) or (x[1]!=0 and x[5]!=0))-1\n",
    "def equa_element_3(x):\n",
    "    return ((x[2]==0 and x[6]==0) or (x[2]!=0 and x[6]!=0))-1\n",
    "def equa_element_4(x):\n",
    "    return ((x[3]==0 and x[7]==0) or (x[3]!=0 and x[7]!=0))-1\n",
    "def uneq_element_2(x):\n",
    "    return ((x[1]!=x[0] and x[1]!=x[2] and x[1]!=x[3]) or (x[1]==0 and x[2]==0 and x[3]==0))-1\n",
    "def uneq_element_3(x):\n",
    "    return ((x[2]!=x[0] and x[2]!=x[1] and x[2]!=x[3]) or (x[1]==0 and x[2]==0 and x[3]==0) or (x[2]==0 and x[3]==0))-1\n",
    "def uneq_element_4(x):\n",
    "    return ((x[3]!=x[0] and x[3]!=x[1] and x[3]!=x[2]) or (x[1]==0 and x[2]==0 and x[3]==0) or (x[2]==0 and x[3]==0))-1\n",
    "def must_need_element_Ir_Ru(x):\n",
    "    return ((1 in [x[0],x[1],x[2],x[3]]) or (2 in [x[0],x[1],x[2],x[3]]))-1\n",
    "def sum_prop(x):\n",
    "    return 100  -x[4] -x[5] -x[6] -x[7]\n",
    "####for variance###\n",
    "def dopants_should_not_be_in_first_two(x):\n",
    "    return ((x[0] in [1,2,4,5,6,10,15,16]) and (x[1] in [1,2,4,5,6,10,15,16]))-1\n",
    "\n",
    "constraint_eq_best = [\n",
    "    sum_prop,\n",
    "    equa_element_2,\n",
    "    equa_element_3,\n",
    "    equa_element_4,\n",
    "    uneq_element_2,\n",
    "    uneq_element_3,\n",
    "    uneq_element_4,\n",
    "    must_need_element_Ir_Ru\n",
    "    ]\n",
    "constraint_eq_variance = [\n",
    "    sum_prop,\n",
    "    equa_element_2,\n",
    "    equa_element_3,\n",
    "    equa_element_4,\n",
    "    uneq_element_2,\n",
    "    uneq_element_3,\n",
    "    uneq_element_4,\n",
    "    must_need_element_Ir_Ru,\n",
    "    dopants_should_not_be_in_first_two\n",
    "    ]\n",
    "constraint_ueq = [\n",
    "    lambda x: x[7] - x[6],\n",
    "    lambda x: x[6] - x[5],\n",
    "    lambda x: x[5] - x[4],\n",
    "    lambda x: x[10]+x[11]-1.9\n",
    "    ]\n",
    "def generate_costly_function(task_type='io_costly'):\n",
    "    # generate a high cost function to test all the modes\n",
    "    # cost_type can be 'io_costly' or 'cpu_costly'\n",
    "    if task_type == 'io_costly':\n",
    "        def costly_function():\n",
    "            time.sleep(0.1)\n",
    "            return 1\n",
    "    else:\n",
    "        def costly_function():\n",
    "            n = 10000\n",
    "            step1 = [np.log(i + 1) for i in range(n)]\n",
    "            step2 = [np.power(i, 1.1) for i in range(n)]\n",
    "            step3 = sum(step1) + sum(step2)\n",
    "            return step3\n",
    "\n",
    "    return costly_function\n",
    "def predict_func_OVP_400(p):\n",
    "    ele_1,ele_2,ele_3,ele_4,prop_1,prop_2,prop_3,prop_4,hydro_temp,hydro_time,hydro_reduc,hydro_ball,post_process=p\n",
    "    test_info=[0.5,0,0,0,5,1]\n",
    "    ele_info=element_information[element_list[int(ele_1)]]+element_information[element_list[int(ele_2)]]+element_information[element_list[int(ele_3)]]+element_information[element_list[int(ele_4)]]\n",
    "    prop_info=[prop_1,prop_2,prop_3,prop_4]\n",
    "    hydrothermal_info=[hydro_temp,hydro_time,1]+[0,hydro_reduc,hydro_ball]\n",
    "    annealing_info=[400,120,0,0,0,post_process]\n",
    "    ###合并制备条件\n",
    "    info_all=ele_info+prop_info+hydrothermal_info+annealing_info+test_info\n",
    "    input_array=np.array(info_all)\n",
    "    input_array=input_array.reshape(1, -1)\n",
    "    input_array=pd.DataFrame(input_array,columns=data_input_full.columns)\n",
    "    #####预测\n",
    "    pred_list=[]\n",
    "    pred_list.append(SVR_full.predict(input_array)[0])\n",
    "    pred_list.append(KNR_full.predict(input_array)[0])\n",
    "    pred_list.append(LGBM_full.predict(input_array)[0])\n",
    "    pred_list.append(XG_full.predict(input_array)[0])\n",
    "    pred_list.append(CAT_full.predict(input_array)[0])\n",
    "    pred_list.append(GB_full.predict(input_array)[0])\n",
    "    pred_list.append(RF_full.predict(input_array)[0])\n",
    "    pred_list.append(DT_full.predict(input_array)[0])\n",
    "    pred_list.append(ADA_full.predict(input_array)[0])\n",
    "    pred_list.append(ANN_1layer.predict(input_array)[0])\n",
    "    pred_list.append(ANN_2layer.predict(input_array)[0])\n",
    "    ####weighted_score####\n",
    "    test_score_list=[SVR_full_score,\n",
    "    KNR_full_score,\n",
    "    LGBM_full_score,\n",
    "    XG_full_score,\n",
    "    CAT_full_score,\n",
    "    GB_full_score,\n",
    "    RF_full_score,\n",
    "    DT_full_score,\n",
    "    ADA_full_score,\n",
    "    ANN_1layer_score,\n",
    "    ANN_2layer_score]\n",
    "\n",
    "    weighted_score_list=test_score_list/max(test_score_list)\n",
    "    for i in range (0,len(weighted_score_list)):\n",
    "        weighted_score_list[i]=1/weighted_score_list[i]\n",
    "    weighted_pred_list=[]\n",
    "    for i in range(0,len(weighted_score_list)):\n",
    "        weighted_pred_list.append(weighted_score_list[i]*pred_list[i])\n",
    "    return mean(weighted_pred_list)\n",
    "\n",
    "def predict_func_variance_400(p):\n",
    "    ele_1,ele_2,ele_3,ele_4,prop_1,prop_2,prop_3,prop_4,hydro_temp,hydro_time,hydro_reduc,hydro_ball,post_process=p\n",
    "    test_info=[0.5,0,0,0,5,1]\n",
    "    ele_info=element_information[element_list[int(ele_1)]]+element_information[element_list[int(ele_2)]]+element_information[element_list[int(ele_3)]]+element_information[element_list[int(ele_4)]]\n",
    "    prop_info=[prop_1,prop_2,prop_3,prop_4]\n",
    "    hydrothermal_info=[hydro_temp,hydro_time,1]+[0,hydro_reduc,hydro_ball]\n",
    "    annealing_info=[400,120,0,0,0,post_process]\n",
    "    ###合并制备条件\n",
    "    info_all=ele_info+prop_info+hydrothermal_info+annealing_info+test_info\n",
    "    input_array=np.array(info_all)\n",
    "    input_array=input_array.reshape(1, -1)\n",
    "    input_array=pd.DataFrame(input_array,columns=data_input_full.columns)\n",
    "    #####预测\n",
    "    pred_list=[]\n",
    "    pred_list.append(SVR_full.predict(input_array)[0])\n",
    "    pred_list.append(KNR_full.predict(input_array)[0])\n",
    "    pred_list.append(LGBM_full.predict(input_array)[0])\n",
    "    pred_list.append(XG_full.predict(input_array)[0])\n",
    "    pred_list.append(CAT_full.predict(input_array)[0])\n",
    "    pred_list.append(GB_full.predict(input_array)[0])\n",
    "    pred_list.append(RF_full.predict(input_array)[0])\n",
    "    pred_list.append(DT_full.predict(input_array)[0])\n",
    "    pred_list.append(ADA_full.predict(input_array)[0])\n",
    "    pred_list.append(ANN_1layer.predict(input_array)[0])\n",
    "    pred_list.append(ANN_2layer.predict(input_array)[0])\n",
    "#     print(pred_list)\n",
    "    ####weighted_score####\n",
    "    test_score_list=[SVR_full_score,\n",
    "    KNR_full_score,\n",
    "    LGBM_full_score,\n",
    "    XG_full_score,\n",
    "    CAT_full_score,\n",
    "    GB_full_score,\n",
    "    RF_full_score,\n",
    "    DT_full_score,\n",
    "    ADA_full_score,\n",
    "    ANN_1layer_score,\n",
    "    ANN_2layer_score]\n",
    "\n",
    "    weighted_score_list=test_score_list/max(test_score_list)\n",
    "    for i in range (0,len(weighted_score_list)):\n",
    "        weighted_score_list[i]=1/weighted_score_list[i]\n",
    "    weighted_pred_list=[]\n",
    "    for i in range(0,len(weighted_score_list)):\n",
    "        weighted_pred_list.append(weighted_score_list[i]*pred_list[i])\n",
    "    return -np.var(pred_list)\n",
    "\n",
    "def predict_func_OVP_500(p):\n",
    "    ele_1,ele_2,ele_3,ele_4,prop_1,prop_2,prop_3,prop_4,hydro_temp,hydro_time,hydro_reduc,hydro_ball,post_process=p\n",
    "    test_info=[0.5,0,0,0,5,1]\n",
    "    ele_info=element_information[element_list[int(ele_1)]]+element_information[element_list[int(ele_2)]]+element_information[element_list[int(ele_3)]]+element_information[element_list[int(ele_4)]]\n",
    "    prop_info=[prop_1,prop_2,prop_3,prop_4]\n",
    "    hydrothermal_info=[hydro_temp,hydro_time,1]+[0,hydro_reduc,hydro_ball]\n",
    "    annealing_info=[500,120,0,0,0,post_process]\n",
    "    ###合并制备条件\n",
    "    info_all=ele_info+prop_info+hydrothermal_info+annealing_info+test_info\n",
    "    input_array=np.array(info_all)\n",
    "    input_array=input_array.reshape(1, -1)\n",
    "    input_array=pd.DataFrame(input_array,columns=data_input_full.columns)\n",
    "    #####预测\n",
    "    pred_list=[]\n",
    "    pred_list.append(SVR_full.predict(input_array)[0])\n",
    "    pred_list.append(KNR_full.predict(input_array)[0])\n",
    "    pred_list.append(LGBM_full.predict(input_array)[0])\n",
    "    pred_list.append(XG_full.predict(input_array)[0])\n",
    "    pred_list.append(CAT_full.predict(input_array)[0])\n",
    "    pred_list.append(GB_full.predict(input_array)[0])\n",
    "    pred_list.append(RF_full.predict(input_array)[0])\n",
    "    pred_list.append(DT_full.predict(input_array)[0])\n",
    "    pred_list.append(ADA_full.predict(input_array)[0])\n",
    "    pred_list.append(ANN_1layer.predict(input_array)[0])\n",
    "    pred_list.append(ANN_2layer.predict(input_array)[0])\n",
    "    ####weighted_score####\n",
    "    test_score_list=[SVR_full_score,\n",
    "    KNR_full_score,\n",
    "    LGBM_full_score,\n",
    "    XG_full_score,\n",
    "    CAT_full_score,\n",
    "    GB_full_score,\n",
    "    RF_full_score,\n",
    "    DT_full_score,\n",
    "    ADA_full_score,\n",
    "    ANN_1layer_score,\n",
    "    ANN_2layer_score]\n",
    "\n",
    "    weighted_score_list=test_score_list/max(test_score_list)\n",
    "    for i in range (0,len(weighted_score_list)):\n",
    "        weighted_score_list[i]=1/weighted_score_list[i]\n",
    "    weighted_pred_list=[]\n",
    "    for i in range(0,len(weighted_score_list)):\n",
    "        weighted_pred_list.append(weighted_score_list[i]*pred_list[i])\n",
    "    return mean(weighted_pred_list)\n",
    "\n",
    "def predict_func_variance_500(p):\n",
    "    ele_1,ele_2,ele_3,ele_4,prop_1,prop_2,prop_3,prop_4,hydro_temp,hydro_time,hydro_reduc,hydro_ball,post_process=p\n",
    "    test_info=[0.5,0,0,0,5,1]\n",
    "    ele_info=element_information[element_list[int(ele_1)]]+element_information[element_list[int(ele_2)]]+element_information[element_list[int(ele_3)]]+element_information[element_list[int(ele_4)]]\n",
    "    prop_info=[prop_1,prop_2,prop_3,prop_4]\n",
    "    hydrothermal_info=[hydro_temp,hydro_time,1]+[0,hydro_reduc,hydro_ball]\n",
    "    annealing_info=[500,120,0,0,0,post_process]\n",
    "    ###合并制备条件\n",
    "    info_all=ele_info+prop_info+hydrothermal_info+annealing_info+test_info\n",
    "    input_array=np.array(info_all)\n",
    "    input_array=input_array.reshape(1, -1)\n",
    "    input_array=pd.DataFrame(input_array,columns=data_input_full.columns)\n",
    "    #####预测\n",
    "    pred_list=[]\n",
    "    pred_list.append(SVR_full.predict(input_array)[0])\n",
    "    pred_list.append(KNR_full.predict(input_array)[0])\n",
    "    pred_list.append(LGBM_full.predict(input_array)[0])\n",
    "    pred_list.append(XG_full.predict(input_array)[0])\n",
    "    pred_list.append(CAT_full.predict(input_array)[0])\n",
    "    pred_list.append(GB_full.predict(input_array)[0])\n",
    "    pred_list.append(RF_full.predict(input_array)[0])\n",
    "    pred_list.append(DT_full.predict(input_array)[0])\n",
    "    pred_list.append(ADA_full.predict(input_array)[0])\n",
    "    pred_list.append(ANN_1layer.predict(input_array)[0])\n",
    "    pred_list.append(ANN_2layer.predict(input_array)[0])\n",
    "#     print(pred_list)\n",
    "    ####weighted_score####\n",
    "    test_score_list=[SVR_full_score,\n",
    "    KNR_full_score,\n",
    "    LGBM_full_score,\n",
    "    XG_full_score,\n",
    "    CAT_full_score,\n",
    "    GB_full_score,\n",
    "    RF_full_score,\n",
    "    DT_full_score,\n",
    "    ADA_full_score,\n",
    "    ANN_1layer_score,\n",
    "    ANN_2layer_score]\n",
    "\n",
    "    weighted_score_list=test_score_list/max(test_score_list)\n",
    "    for i in range (0,len(weighted_score_list)):\n",
    "        weighted_score_list[i]=1/weighted_score_list[i]\n",
    "    weighted_pred_list=[]\n",
    "    for i in range(0,len(weighted_score_list)):\n",
    "        weighted_pred_list.append(weighted_score_list[i]*pred_list[i])\n",
    "    return -np.var(pred_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a21c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "###run GA to predict highest variance 400 degree\n",
    "for i in range(0,48):\n",
    "    task_type='cpu_costly'\n",
    "    costly_function = generate_costly_function(task_type=task_type)\n",
    "    # to use the vectorization mode, the function itself should support the mode.\n",
    "    mode = 'common'\n",
    "    # mode ='vectorization'\n",
    "    set_run_mode(predict_func_variance_400, mode)\n",
    "    ga = GA(func=predict_func_variance_400, n_dim=13, size_pop=3000, max_iter=50,prob_mut=0.01,lb=[1, 0, 0, 0, 0, 0, 0, 0,25, 120, 0, 0, 0], ub=[34, 34, 34, 34, 100, 100, 100, 100, 60,1440,1,1,1],\n",
    "            constraint_eq=constraint_eq_variance, constraint_ueq=constraint_ueq,precision=[1,1,1,1,1,1,1,1,1,10,1,1,1])\n",
    "    start_time = datetime.datetime.now()\n",
    "    best_x, best_y = ga.run()\n",
    "    print('best_x:', best_x, '\\n', 'best_y:', best_y)\n",
    "    print('on {task_type} task,use {mode} mode, costs {time_costs}s'\n",
    "          .format(task_type=task_type, mode=mode,\n",
    "                  time_costs=(datetime.datetime.now() - start_time).total_seconds()))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434c10cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "###run GA to predict highest variance 500 degree\n",
    "for i in range(0,48):\n",
    "    task_type='cpu_costly'\n",
    "    costly_function = generate_costly_function(task_type=task_type)\n",
    "    # to use the vectorization mode, the function itself should support the mode.\n",
    "    mode = 'common'\n",
    "    # mode ='vectorization'\n",
    "    set_run_mode(predict_func_variance_500, mode)\n",
    "    ga = GA(func=predict_func_variance_500, n_dim=13, size_pop=3000, max_iter=50,prob_mut=0.01,lb=[1, 0, 0, 0, 0, 0, 0, 0,25, 120, 0, 0, 0], ub=[34, 34, 34, 34, 100, 100, 100, 100, 60,1440,1,1,1],\n",
    "            constraint_eq=constraint_eq_variance, constraint_ueq=constraint_ueq,precision=[1,1,1,1,1,1,1,1,1,10,1,1,1])\n",
    "    start_time = datetime.datetime.now()\n",
    "    best_x, best_y = ga.run()\n",
    "    print('best_x:', best_x, '\\n', 'best_y:', best_y)\n",
    "    print('on {task_type} task,use {mode} mode, costs {time_costs}s'\n",
    "          .format(task_type=task_type, mode=mode,\n",
    "                  time_costs=(datetime.datetime.now() - start_time).total_seconds()))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88483ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###run GA to predict lowest overpotential 400 degree\n",
    "for i in range(0,48):\n",
    "    task_type='cpu_costly'\n",
    "    costly_function = generate_costly_function(task_type=task_type)\n",
    "    # to use the vectorization mode, the function itself should support the mode.\n",
    "    mode = 'common'\n",
    "    # mode ='vectorization'\n",
    "    set_run_mode(predict_func_OVP_400, mode)\n",
    "    ga = GA(func=predict_func_OVP_400, n_dim=13, size_pop=3000, max_iter=50,prob_mut=0.01,lb=[1, 0, 0, 0, 0, 0, 0, 0,25, 120, 0, 0, 0], ub=[34, 34, 34, 34, 100, 100, 100, 100, 60,1440,1,1,1],\n",
    "            constraint_eq=constraint_eq_variance, constraint_ueq=constraint_ueq,precision=[1,1,1,1,1,1,1,1,1,10,1,1,1])\n",
    "    start_time = datetime.datetime.now()\n",
    "    best_x, best_y = ga.run()\n",
    "    print('best_x:', best_x, '\\n', 'best_y:', best_y)\n",
    "    print('on {task_type} task,use {mode} mode, costs {time_costs}s'\n",
    "          .format(task_type=task_type, mode=mode,\n",
    "                  time_costs=(datetime.datetime.now() - start_time).total_seconds()))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d67aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###run GA to predict lowest overpotential 500 degree\n",
    "for i in range(0,48):\n",
    "    task_type='cpu_costly'\n",
    "    costly_function = generate_costly_function(task_type=task_type)\n",
    "    # to use the vectorization mode, the function itself should support the mode.\n",
    "    mode = 'common'\n",
    "    # mode ='vectorization'\n",
    "    set_run_mode(predict_func_OVP_500, mode)\n",
    "    ga = GA(func=predict_func_OVP_500, n_dim=13, size_pop=3000, max_iter=50,prob_mut=0.01,lb=[1, 0, 0, 0, 0, 0, 0, 0,25, 120, 0, 0, 0], ub=[34, 34, 34, 34, 100, 100, 100, 100, 60,1440,1,1,1],\n",
    "            constraint_eq=constraint_eq_variance, constraint_ueq=constraint_ueq,precision=[1,1,1,1,1,1,1,1,1,10,1,1,1])\n",
    "    start_time = datetime.datetime.now()\n",
    "    best_x, best_y = ga.run()\n",
    "    print('best_x:', best_x, '\\n', 'best_y:', best_y)\n",
    "    print('on {task_type} task,use {mode} mode, costs {time_costs}s'\n",
    "          .format(task_type=task_type, mode=mode,\n",
    "                  time_costs=(datetime.datetime.now() - start_time).total_seconds()))\n",
    "    i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
