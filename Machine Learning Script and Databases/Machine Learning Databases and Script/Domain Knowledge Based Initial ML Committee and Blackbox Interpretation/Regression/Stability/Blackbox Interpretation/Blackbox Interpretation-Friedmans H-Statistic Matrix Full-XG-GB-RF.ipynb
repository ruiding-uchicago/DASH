{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aacfec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########import packages##########\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import ensemble\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "seed=911\n",
    "###########import packages##########\n",
    "import catboost\n",
    "import xgboost\n",
    "import lightgbm\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import *\n",
    "import pickle\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import  *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28640cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########wrapping root mean square error for later calls##########\n",
    "def compute_mae_mse_rmse(target,prediction):\n",
    "    error = []\n",
    "    for i in range(len(target)):\n",
    "        error.append(target[i] - prediction[i])\n",
    "    squaredError = []\n",
    "    absError = []\n",
    "    for val in error:\n",
    "        squaredError.append(val * val)  # target-prediction之差平方\n",
    "        absError.append(abs(val))  # 误差绝对值\n",
    "    mae=sum(absError)/len(absError)  # 平均绝对误差MAE\n",
    "    mse=sum(squaredError)/len(squaredError)  # 均方误差MSE\n",
    "    RMSE=np.sqrt(sum(squaredError)/len(squaredError))\n",
    "    R2=r2_score(target,prediction)\n",
    "    return mae,mse,RMSE,R2\n",
    "def gridsearch(model,param,algorithm_name,X_train,y_train,X_test,y_test):\n",
    "    grid = GridSearchCV(model,param_grid=param,scoring='neg_mean_absolute_error',cv=10,n_jobs=-1,verbose=-1)\n",
    "    grid.fit(X_train,y_train)\n",
    "    best_model=grid.best_estimator_\n",
    "    ####Train####\n",
    "    prediction_train = best_model.predict(X_train)\n",
    "    real_train=y_train.values\n",
    "    prediction_train_series=pd.Series(prediction_train)\n",
    "    real_train_series=pd.Series(real_train)\n",
    "    corr_ann_train = round(prediction_train_series.corr(real_train_series), 5)\n",
    "    error_val_train= compute_mae_mse_rmse(prediction_train,real_train)\n",
    "    ####test####\n",
    "    prediction_test = best_model.predict(X_test)\n",
    "    real_test=y_test.values\n",
    "    prediction_test_series=pd.Series(prediction_test)\n",
    "    real_test_series=pd.Series(real_test)\n",
    "    corr_ann_test = round(prediction_test_series.corr(real_test_series), 5)\n",
    "    error_val_test= compute_mae_mse_rmse(prediction_test,real_test)\n",
    "    print(algorithm_name)\n",
    "    best_score=grid.best_score_\n",
    "    print('Best Regressor:',grid.best_params_,'Best Score:', best_score)\n",
    "    print(error_val_test)\n",
    "    print('R2 TEST',error_val_test[3])\n",
    "    fig=plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    x_y_x=np.arange(-4,4,0.01)\n",
    "    x_y_y=np.arange(-4,4,0.01)\n",
    "    ax.scatter(prediction_train,real_train,c='blue',label='Train',alpha=0.25)\n",
    "    ax.scatter(prediction_test,real_test,c='red',label='Test',alpha=0.75)\n",
    "    ax.plot(x_y_x,x_y_y,c='black')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Predicted_log_(Decay_Rate) (mV h-1)')\n",
    "    plt.ylabel('Real_log_(Decay_Rate) (mV h-1)')\n",
    "    return best_model,error_val_test[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b43574",
   "metadata": {},
   "outputs": [],
   "source": [
    "fl = open(r'./database_full_st_for_inter.pkl','rb')\n",
    "database_full=pickle.load(fl)\n",
    "data_input_full=database_full.iloc[:,0:55]\n",
    "data_output_full=database_full.iloc[:,55]\n",
    "X_train,X_test,y_train,y_test=train_test_split(data_input_full,data_output_full,test_size=0.1,random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612addb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_XGRegressor=XGBRegressor(random_state=1)\n",
    "param_xg={\n",
    "# 'booster':['gbtree'],\n",
    "# 'learning_rate':[0.001,0.002,0.004,0.005,0.006,0.008,0.01,0.02,0.04,0.06,0.05,0.06,0.08,0.1,0.12,0.14,0.15,0.16,0.18,0.2,0.4,0.5,0.6,0.8,1],\n",
    "# 'n_estimators':[100,200,400],\n",
    "# 'max_depth':[3,5,7,9,11,13,-1],\n",
    "# 'subsample':[0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1],\n",
    "# 'reg_alpha':[0,0.001,0.01,0.0001,0.00001],\n",
    "# 'reg_lambda':[0,0.001,0.01,0.0001,0.00001]\n",
    "'booster': ['gbtree'], 'learning_rate': [0.05], 'max_depth': [11], 'n_estimators': [100], 'reg_alpha': [1e-05], 'reg_lambda': [0], 'subsample': [0.45 ]   \n",
    "}\n",
    "XG_full,XG_full_score=gridsearch(model_XGRegressor,param_xg,'XGBoost',X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6279757",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########RandomForest gridsearch CV for best hyperparameter##########\n",
    "model_RandomForestRegressor = ensemble.RandomForestRegressor(random_state=1)\n",
    "###########defining the parameters dictionary##########\n",
    "param_RF = {\n",
    "# 'n_estimators':[50,100,200,400,None],\n",
    "# 'max_depth':[3,5,7,9,11,None],\n",
    "# 'criterion':['mse','mae'],\n",
    "# 'max_features':['auto','sqrt','log2']\n",
    "'criterion': ['mae'], 'max_depth': [11], 'max_features': ['auto'], 'n_estimators': [400]\n",
    "}\n",
    "RF_full,RF_full_score=gridsearch(model_RandomForestRegressor,param_RF,'Random Forest',X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fcb036",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_GradientBoostingRegressor = ensemble.GradientBoostingRegressor(random_state=1)\n",
    "###########defining the parameters dictionary##########\n",
    "param_GB = {\n",
    "# 'learning_rate':[0.001,0.002,0.004,0.005,0.006,0.008,0.01,0.02,0.04,0.06,0.05,0.06,0.08,0.1,0.12,0.14,0.15,0.16,0.18,0.2,0.4,0.5,0.6,0.8,1],\n",
    "# 'n_estimators':[50,100,200,400],\n",
    "# 'max_depth':[3,5,7,9,11,13,16],\n",
    "# 'criterion':['friedman_mse','mae','mse'],\n",
    "# 'max_features':['auto','sqrt','log2'],\n",
    "# 'loss':['ls', 'lad', 'huber', 'quantile']\n",
    "    'criterion': ['friedman_mse'], 'learning_rate': [0.05], 'loss': ['lad'], 'max_depth': [9], 'max_features': ['auto'], 'n_estimators': [100]\n",
    "}\n",
    "GB_full,GB_full_score=gridsearch(model_GradientBoostingRegressor,param_GB,'GradientBoost',X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb8e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pdpbox.pdp_calc_utils import _calc_ice_lines_inter\n",
    "from pdpbox.pdp import pdp_isolate, PDPInteract\n",
    "from pdpbox.utils import (_check_model, _check_dataset, _check_percentile_range, _check_feature,\n",
    "                    _check_grid_type, _check_memory_limit, _make_list,\n",
    "                    _calc_memory_usage, _get_grids, _get_grid_combos, _check_classes)\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def pdp_multi_interact(model, dataset, model_features, features, \n",
    "                    num_grid_points=None, grid_types=None, percentile_ranges=None, grid_ranges=None, cust_grid_points=None, \n",
    "                    cust_grid_combos=None, use_custom_grid_combos=False,\n",
    "                    memory_limit=0.9, n_jobs=8, predict_kwds=None, data_transformer=None):\n",
    "\n",
    "    def _expand_default(x, default, length):\n",
    "        if x is None:\n",
    "            return [default] * length\n",
    "        return x\n",
    "\n",
    "    def _get_grid_combos(feature_grids, feature_types):\n",
    "        grids = [np.array(list(feature_grid),dtype=np.float16) for feature_grid in feature_grids]\n",
    "        for i in range(len(feature_types)):\n",
    "            if feature_types[i] == 'onehot':\n",
    "                grids[i] = np.eye(len(grids[i])).astype(int).tolist()\n",
    "        return np.stack(np.meshgrid(*grids,copy=bool), -1).reshape(-1, len(grids))\n",
    "\n",
    "    if predict_kwds is None:\n",
    "        predict_kwds = dict()\n",
    "\n",
    "    nr_feats = len(features)\n",
    "\n",
    "    # check function inputs\n",
    "    n_classes, predict = _check_model(model=model)\n",
    "    _check_dataset(df=dataset)\n",
    "    _dataset = dataset.copy()\n",
    "\n",
    "    # prepare the grid\n",
    "    pdp_isolate_outs = []\n",
    "    if use_custom_grid_combos:\n",
    "        grid_combos = cust_grid_combos\n",
    "        feature_grids = []\n",
    "        feature_types = []\n",
    "    else:\n",
    "        num_grid_points = _expand_default(x=num_grid_points, default=10, length=nr_feats)\n",
    "        grid_types = _expand_default(x=grid_types, default='percentile', length=nr_feats)\n",
    "        for i in range(nr_feats):\n",
    "            _check_grid_type(grid_type=grid_types[i])\n",
    "\n",
    "        percentile_ranges = _expand_default(x=percentile_ranges, default=None, length=nr_feats)\n",
    "        for i in range(nr_feats):\n",
    "            _check_percentile_range(percentile_range=percentile_ranges[i])\n",
    "\n",
    "        grid_ranges = _expand_default(x=grid_ranges, default=None, length=nr_feats)\n",
    "        cust_grid_points = _expand_default(x=cust_grid_points, default=None, length=nr_feats)\n",
    "\n",
    "        _check_memory_limit(memory_limit=memory_limit)\n",
    "\n",
    "        pdp_isolate_outs = []\n",
    "        for idx in range(nr_feats):\n",
    "            pdp_isolate_out = pdp_isolate(\n",
    "                model=model, dataset=_dataset, model_features=model_features, feature=features[idx],\n",
    "                num_grid_points=num_grid_points[idx], grid_type=grid_types[idx], percentile_range=percentile_ranges[idx],\n",
    "                grid_range=grid_ranges[idx], cust_grid_points=cust_grid_points[idx], memory_limit=memory_limit,\n",
    "                n_jobs=n_jobs, predict_kwds=predict_kwds, data_transformer=data_transformer)\n",
    "            pdp_isolate_outs.append(pdp_isolate_out)\n",
    "\n",
    "        if n_classes > 2:\n",
    "            feature_grids = [pdp_isolate_outs[i][0].feature_grids for i in range(nr_feats)]\n",
    "            feature_types = [pdp_isolate_outs[i][0].feature_type  for i in range(nr_feats)]\n",
    "        else:\n",
    "            feature_grids = [pdp_isolate_outs[i].feature_grids for i in range(nr_feats)]\n",
    "            feature_types = [pdp_isolate_outs[i].feature_type  for i in range(nr_feats)]\n",
    "\n",
    "        grid_combos = _get_grid_combos(feature_grids, feature_types)\n",
    "\n",
    "    feature_list = []\n",
    "    for i in range(nr_feats):\n",
    "        feature_list.extend(_make_list(features[i]))\n",
    "\n",
    "    # Parallel calculate ICE lines\n",
    "    true_n_jobs = _calc_memory_usage(\n",
    "        df=_dataset, total_units=len(grid_combos), n_jobs=n_jobs, memory_limit=memory_limit)\n",
    "\n",
    "    grid_results = Parallel(n_jobs=true_n_jobs)(delayed(_calc_ice_lines_inter)(\n",
    "        grid_combo, data=_dataset, model=model, model_features=model_features, n_classes=n_classes,\n",
    "        feature_list=feature_list, predict_kwds=predict_kwds, data_transformer=data_transformer)\n",
    "                                                for grid_combo in grid_combos)\n",
    "\n",
    "    ice_lines = pd.concat(grid_results, axis=0).reset_index(drop=True)\n",
    "    pdp = ice_lines.groupby(feature_list, as_index=False).mean()\n",
    "\n",
    "    # combine the final results\n",
    "    pdp_interact_params = {'n_classes': n_classes, \n",
    "                        'features': features, \n",
    "                        'feature_types': feature_types,\n",
    "                        'feature_grids': feature_grids}\n",
    "    if n_classes > 2:\n",
    "        pdp_interact_out = []\n",
    "        for n_class in range(n_classes):\n",
    "            _pdp = pdp[feature_list + ['class_%d_preds' % n_class]].rename(\n",
    "                columns={'class_%d_preds' % n_class: 'preds'})\n",
    "            pdp_interact_out.append(\n",
    "                PDPInteract(which_class=n_class,\n",
    "                            pdp_isolate_outs=[pdp_isolate_outs[i][n_class] for i in range(nr_feats)],\n",
    "                            pdp=_pdp, **pdp_interact_params))\n",
    "    else:\n",
    "        pdp_interact_out = PDPInteract(\n",
    "            which_class=None, pdp_isolate_outs=pdp_isolate_outs, pdp=pdp, **pdp_interact_params)\n",
    "\n",
    "    return pdp_interact_out\n",
    "print('ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a368ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center(arr): return arr - np.mean(arr)\n",
    "import itertools\n",
    "def compute_f_vals(mdl, X, features, selectedfeatures, num_grid_points=10, use_data_grid=False):\n",
    "    f_vals = {}\n",
    "    data_grid = None\n",
    "    if use_data_grid:\n",
    "        data_grid = X[selectedfeatures].values\n",
    "    # Calculate partial dependencies for full feature set\n",
    "    p_full = pdp_multi_interact(mdl, X, features, selectedfeatures, \n",
    "                                num_grid_points=[num_grid_points] * len(selectedfeatures),\n",
    "                                cust_grid_combos=data_grid,\n",
    "                                use_custom_grid_combos=use_data_grid)\n",
    "    f_vals[tuple(selectedfeatures)] = center(p_full.pdp.preds.values)\n",
    "    grid = p_full.pdp.drop('preds', axis=1)\n",
    "    # Calculate partial dependencies for [1..SFL-1]\n",
    "    for n in range(1, len(selectedfeatures)):\n",
    "        for subsetfeatures in itertools.combinations(selectedfeatures, n):\n",
    "            if use_data_grid:\n",
    "                data_grid = X[list(subsetfeatures)].values\n",
    "            p_partial = pdp_multi_interact(mdl, X, features, subsetfeatures, \n",
    "                                        num_grid_points=[num_grid_points] * len(selectedfeatures),\n",
    "                                        cust_grid_combos=data_grid,\n",
    "                                        use_custom_grid_combos=use_data_grid)\n",
    "            p_joined = pd.merge(grid, p_partial.pdp, how='left')\n",
    "            f_vals[tuple(subsetfeatures)] = center(p_joined.preds.values)\n",
    "    return f_vals\n",
    "def compute_h_val(f_vals, selectedfeatures):\n",
    "    denom_els = f_vals[tuple(selectedfeatures)].copy()\n",
    "    numer_els = f_vals[tuple(selectedfeatures)].copy()\n",
    "    sign = -1.0\n",
    "    for n in range(len(selectedfeatures)-1, 0, -1):\n",
    "        for subfeatures in itertools.combinations(selectedfeatures, n):\n",
    "            print(tuple(subfeatures))\n",
    "            numer_els += sign * f_vals[tuple(subfeatures)]\n",
    "        sign *= -1.0\n",
    "    numer = np.sum(numer_els**2)\n",
    "    denom = np.sum(denom_els**2)\n",
    "    return math.sqrt(numer/denom) if numer < denom else np.nan\n",
    "def compute_h_val_any(f_vals, allfeatures, selectedfeature):\n",
    "    otherfeatures = list(allfeatures)\n",
    "    otherfeatures.remove(selectedfeature)\n",
    "    denom_els = f_vals[tuple(allfeatures)].copy()\n",
    "    numer_els = denom_els.copy()\n",
    "    numer_els -= f_vals[(selectedfeature,)]\n",
    "    numer_els -= f_vals[tuple(otherfeatures)]\n",
    "    numer = np.sum(numer_els**2)\n",
    "    denom = np.sum(denom_els**2)\n",
    "    return math.sqrt(numer/denom) if numer < denom else np.nan\n",
    "def compute_interactions(model,X_train,feature_all,feature_select_list):  \n",
    "    result_dict={}\n",
    "    for i in range(len(feature_select_list)):\n",
    "        for j in range(len(feature_select_list)):\n",
    "            if i<j :\n",
    "                print(i,j)\n",
    "                try:\n",
    "                    current_features=[feature_select_list[i],feature_select_list[j]]\n",
    "                    f_vals=compute_f_vals(model, X_train, feature_all,current_features) \n",
    "                    result_dict[tuple(current_features)]=compute_h_val(f_vals,current_features)\n",
    "                except:\n",
    "                    result_dict[tuple(current_features)]=0\n",
    "                print(result_dict[tuple(current_features)])\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df5e989",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "XG_DICT=compute_interactions(XG_full,data_input_full,data_input_full.columns,list(data_input_full.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cbbcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_DICT=compute_interactions(RF_full,data_input_full,data_input_full.columns,list(data_input_full.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61434325",
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_DICT=compute_interactions(GB_full,data_input_full,data_input_full.columns,list(data_input_full.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4613ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_matrix_weighted(target_dict,target_score):\n",
    "    df=pd.DataFrame(columns=data_input_full.columns,index=data_input_full.columns)\n",
    "    for each in target_dict:\n",
    "        df.loc[each[0],each[1]]=target_dict[each]*target_score\n",
    "        df.loc[each[1],each[0]]=target_dict[each]*target_score\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7465a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "XG_DF=construct_matrix_weighted(XG_DICT,XG_full_score)\n",
    "RF_DF=construct_matrix_weighted(RF_DICT,RF_full_score)\n",
    "GB_DF=construct_matrix_weighted(GB_DICT,GB_full_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70403616",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GB_DF.to_csv('INTERACTION_GB_FULL.csv')\n",
    "RF_DF.to_csv('INTERACTION_RF_FULL.csv')\n",
    "XG_DF.to_csv('INTERACTION_XG_FULL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e7d378",
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_DF=pd.read_csv('INTERACTION_GB_FULL.csv',index_col=0)\n",
    "RF_DF=pd.read_csv('INTERACTION_RF_FULL.csv',index_col=0)\n",
    "XG_DF=pd.read_csv('INTERACTION_XG_FULL.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b814b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Weighted_Matrix=(XG_DF+RF_DF+GB_DF)/(XG_full_score+RF_full_score+GB_full_score)\n",
    "Weighted_Matrix=Weighted_Matrix.fillna(0)\n",
    "Weighted_Matrix=Weighted_Matrix/Weighted_Matrix.max().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43fcc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "Weighted_Matrix.to_csv('INTERACTION_FULL_WEIGHTED.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0cbd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "f, ax= plt.subplots(figsize = (16, 16))\n",
    "sns.set(font_scale=1)\n",
    "ax=sns.heatmap(Weighted_Matrix,annot=False, vmax=1,vmin = 0, xticklabels= True, yticklabels= True, square=True, cmap=\"gist_heat_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6209a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Element_M=Weighted_Matrix.iloc[0:36,0:36]\n",
    "Synthesis_M=Weighted_Matrix.iloc[36:55,36:55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fe2f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1, ax1= plt.subplots(figsize = (16, 16))\n",
    "# sns.set(font_scale=2)\n",
    "ax1=sns.heatmap(Element_M,annot=False, vmax=1,vmin = 0, xticklabels= True, yticklabels= True, square=True, cmap=\"gist_heat_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfc9c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2, ax2= plt.subplots(figsize = (16, 16))\n",
    "# sns.set(font_scale=2)\n",
    "ax2=sns.heatmap(Synthesis_M,annot=False, vmax=1,vmin = 0, xticklabels= True, yticklabels= True, square=True, cmap=\"gist_heat_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0be41df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdpbox",
   "language": "python",
   "name": "pdpbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
